{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./jobert-vocab.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\"data/\").glob(\"*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    vocab_file=None,\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False,  # Must be False if cased model\n",
    "    lowercase=False,\n",
    "    wordpieces_prefix=\"##\")\n",
    "\n",
    "min_frequency = 5\n",
    "vocab_size    = 15000\n",
    "limit_alphabet= 6000\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=vocab_size, min_frequency=5, special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])\n",
    "tokenizer.train(\n",
    "    files=paths,\n",
    "    vocab_size=vocab_size,  # 사전의 최대 크기, 32000\n",
    "    min_frequency=min_frequency,  # 단어의 최소 발생 빈도, 3\n",
    "    show_progress=True,\n",
    "    limit_alphabet=limit_alphabet,\n",
    "    special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]',\n",
    "                    '[BOS]', '[EOS]', '[UNK0]', '[UNK1]', '[UNK2]', '[UNK3]', '[UNK4]', '[UNK5]', '[UNK6]', '[UNK7]', '[UNK8]', '[UNK9]',\n",
    "                    '[unused0]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]',\n",
    "                    '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]',\n",
    "                    '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]',\n",
    "                    '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]',\n",
    "                    '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]',\n",
    "                    '[unused50]', '[unused51]', '[unused52]', '[unused53]', '[unused54]', '[unused55]', '[unused56]', '[unused57]', '[unused58]', '[unused59]',\n",
    "                    '[unused60]', '[unused61]', '[unused62]', '[unused63]', '[unused64]', '[unused65]', '[unused66]', '[unused67]', '[unused68]', '[unused69]',\n",
    "                    '[unused70]', '[unused71]', '[unused72]', '[unused73]', '[unused74]', '[unused75]', '[unused76]', '[unused77]', '[unused78]', '[unused79]',\n",
    "                    '[unused80]', '[unused81]', '[unused82]', '[unused83]', '[unused84]', '[unused85]', '[unused86]', '[unused87]', '[unused88]', '[unused89]',\n",
    "                    '[unused90]', '[unused91]', '[unused92]', '[unused93]', '[unused94]', '[unused95]', '[unused96]', '[unused97]', '[unused98]', '[unused99]'\n",
    "                   ]  # 스페셜 토큰\n",
    ")\n",
    "# Save files to disk\n",
    "tokenizer.save(\".\", \"jobert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "No such file or directory (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e96f2c829bcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"jobert.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tokenizers/implementations/base_tokenizer.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, directory, name)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mThe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msaved\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"
     ]
    }
   ],
   "source": [
    "tokenizer.save(\"jobert.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_parameters',\n",
       " '_tokenizer',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'enable_padding',\n",
       " 'enable_truncation',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'get_vocab_size',\n",
       " 'id_to_token',\n",
       " 'no_padding',\n",
       " 'no_truncation',\n",
       " 'num_special_tokens_to_add',\n",
       " 'save',\n",
       " 'token_to_id',\n",
       " 'train']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', ',', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.implementations import BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    \"jobert-vocab.txt\",\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False, # Cased 모델 시 False\n",
    "    lowercase=False,\n",
    "    wordpieces_prefix=\"##\"\n",
    "    #\"jobert-merges.txt\",\n",
    ")\n",
    "\"\"\"\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "\"\"\"\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "print(\n",
    "    tokenizer.encode(\"남조선언론 간또조선인대학살만행과 관련한 자료 공개, 현 일본당국의 파렴치한 태도 비판\").tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_parameters',\n",
       " '_tokenizer',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'decode',\n",
       " 'decode_batch',\n",
       " 'enable_padding',\n",
       " 'enable_truncation',\n",
       " 'encode',\n",
       " 'encode_batch',\n",
       " 'get_vocab_size',\n",
       " 'id_to_token',\n",
       " 'no_padding',\n",
       " 'no_truncation',\n",
       " 'num_special_tokens_to_add',\n",
       " 'save',\n",
       " 'token_to_id',\n",
       " 'train']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers\n",
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-4d2e0a759bba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m'c'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "'c' in tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/kcna.txt',\n",
       " 'data/journals.txt',\n",
       " 'data/hakbo.txt',\n",
       " 'data/juchebooks.txt',\n",
       " 'data/chosonmunhak.txt',\n",
       " 'data/literature.txt',\n",
       " 'data/rodong.txt',\n",
       " 'data/kis.txt',\n",
       " 'data/kji.txt']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 data/kcna.txt\n",
      "🔥 data/journals.txt\n",
      "🔥 data/hakbo.txt\n",
      "🔥 data/juchebooks.txt\n",
      "🔥 data/chosonmunhak.txt\n",
      "🔥 data/literature.txt\n",
      "🔥 data/rodong.txt\n",
      "🔥 data/kis.txt\n",
      "🔥 data/kji.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "src_files = Path(\"./data/\").glob(\"*.txt\")\n",
    "\n",
    "for src_file in src_files:\n",
    "    print(\"🔥\", src_file)\n",
    "    lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "    index = round(len(lines) * 0.8)\n",
    "    while len(lines[index]) != 0:\n",
    "        index += 1\n",
    "    train_file = '\\n'.join(lines[:index])\n",
    "    test_file = '\\n'.join(lines[index+1:])\n",
    "    with open(str(src_file)[:-4] + '-train.txt', 'w', encoding='utf8') as fp:\n",
    "        fp.write(train_file)\n",
    "    with open(str(src_file)[:-4] + '-eval.txt', 'w', encoding='utf8') as fp:\n",
    "        fp.write(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/kcna'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(src_file)[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 data/kcna-train.txt\n",
      "🔥 data/kji-train.txt\n",
      "🔥 data/rodong-train.txt\n",
      "🔥 data/chosonmunhak-train.txt\n",
      "🔥 data/journals-train.txt\n",
      "🔥 data/literature-train.txt\n",
      "🔥 data/hakbo-train.txt\n",
      "🔥 data/juchebooks-train.txt\n",
      "🔥 data/kis-train.txt\n"
     ]
    }
   ],
   "source": [
    "src_files = Path(\"./data/\").glob(\"*-train.txt\")\n",
    "agg_data = ''\n",
    "for src_file in src_files:\n",
    "    print(\"🔥\", src_file)\n",
    "    content = src_file.read_text(encoding=\"utf-8\")\n",
    "    agg_data += content + '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train.txt', 'w', encoding='utf8') as fp:\n",
    "    fp.write(agg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"아동문학작가는 우'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import os\n",
    "import string\n",
    "\n",
    "komoran = Komoran()\n",
    "with open('./data/eval.txt', 'r', encoding='utf8') as fp:\n",
    "    lines = fp.read().splitlines()\n",
    "\n",
    "tokenized = []\n",
    "for line in lines:\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        tokenized.append(' '.join([pos[0] for pos in komoran.pos(line) if pos[0] not in string.punctuation])[:256])\n",
    "\n",
    "with open('./data/train.txt', 'r', encoding='utf8') as fp:\n",
    "    lines = fp.read().splitlines()\n",
    "\n",
    "for line in lines:\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        tokenized.append(' '.join([pos[0] for pos in komoran.pos(line) if pos[0] not in string.punctuation])[:256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'허 약자 들 을 위하 아 떡쌀 을 보내 어 주 도록 하 겠 으니 그 들 에게 떡 을 해먹 이 도록 하 아야 하 겠 습니다'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized[-13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'\\n'.join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./data/tokenized.txt', 'w', encoding='utf8') as fp:\n",
    "    fp.write('\\n'.join(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "import os\n",
    "import string\n",
    "\n",
    "komoran = Komoran()\n",
    "with open('./language_model/train.txt', 'r', encoding='utf8') as fp:\n",
    "    lines = fp.read().splitlines()\n",
    "\n",
    "tokenized = []\n",
    "for line in lines:\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    else:\n",
    "        tokenized.append(' '.join([pos[0] for pos in komoran.pos(line) if pos[0] not in string.punctuation]))\n",
    "with open('./language_model/tokenized-train.txt', 'w', encoding='utf8') as fp:\n",
    "    fp.write('\\n'.join(tokenized))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
