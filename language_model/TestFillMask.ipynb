{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NO_PAD_TOKEN_FOR_BATCH_MSG',\n",
       " 'SPECIAL_TOKENS_ATTRIBUTES',\n",
       " 'UNEVEN_SEQUENCES_FOR_BATCH_MSG',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_additional_special_tokens',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_eos_token',\n",
       " '_from_pretrained',\n",
       " '_mask_token',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_sep_token',\n",
       " '_tokenize',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'basic_tokenizer',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'do_basic_tokenize',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'ids_to_tokens',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'num_added_tokens',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_for_tokenization',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'special_tokens_map',\n",
       " 'tokenize',\n",
       " 'truncate_sequences',\n",
       " 'unique_added_tokens_encoder',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size',\n",
       " 'wordpiece_tokenizer']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'c' in tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0930 10:30:41.032662 24936 configuration_utils.py:262] loading configuration file ./jobert\\config.json\n",
      "I0930 10:30:41.033659 24936 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 20839\n",
      "}\n",
      "\n",
      "I0930 10:30:41.034657 24936 tokenization_utils_base.py:1167] Model name './jobert' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming './jobert' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0930 10:30:41.035656 24936 tokenization_utils_base.py:1197] Didn't find file ./jobert\\added_tokens.json. We won't load it.\n",
      "I0930 10:30:41.059591 24936 tokenization_utils_base.py:1197] Didn't find file ./jobert\\tokenizer.json. We won't load it.\n",
      "I0930 10:30:41.061586 24936 tokenization_utils_base.py:1252] loading file ./jobert\\vocab.txt\n",
      "I0930 10:30:41.062582 24936 tokenization_utils_base.py:1252] loading file None\n",
      "I0930 10:30:41.063580 24936 tokenization_utils_base.py:1252] loading file ./jobert\\special_tokens_map.json\n",
      "I0930 10:30:41.064577 24936 tokenization_utils_base.py:1252] loading file ./jobert\\tokenizer_config.json\n",
      "I0930 10:30:41.065575 24936 tokenization_utils_base.py:1252] loading file None\n",
      "I0930 10:30:41.136386 24936 modelcard.py:177] Model card: {\n",
      "  \"caveats_and_recommendations\": {},\n",
      "  \"ethical_considerations\": {},\n",
      "  \"evaluation_data\": {},\n",
      "  \"factors\": {},\n",
      "  \"intended_use\": {},\n",
      "  \"metrics\": {},\n",
      "  \"model_details\": {},\n",
      "  \"quantitative_analyses\": {},\n",
      "  \"training_data\": {}\n",
      "}\n",
      "\n",
      "I0930 10:30:41.137412 24936 configuration_utils.py:262] loading configuration file ./jobert\\config.json\n",
      "I0930 10:30:41.138408 24936 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 20839\n",
      "}\n",
      "\n",
      "I0930 10:30:41.139378 24936 modeling_utils.py:665] loading weights file ./jobert\\pytorch_model.bin\n",
      "W0930 10:30:48.008118 24936 modeling_utils.py:757] Some weights of the model checkpoint at ./jobert were not used when initializing BertForMaskedLM: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "I0930 10:30:48.012108 24936 modeling_utils.py:774] All the weights of BertForMaskedLM were initialized from the model checkpoint at ./jobert.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./jobert\",\n",
    "    tokenizer=\"./jobert\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran\n",
    "komoran = Komoran()\n",
    "def preproc(sentence):\n",
    "    return ' '.join([_ for _ in komoran.morphs(sentence)]).replace('MASK', '[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask.topk = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] 장내 에 는 떠나가 ㄹ 듯 하 ㄴ 웃음 의 목소리 가 아 지나 갔 다. [SEP]',\n",
       "  'score': 0.299342542886734,\n",
       "  'token': 15088,\n",
       "  'token_str': '목소리'},\n",
       " {'sequence': '[CLS] 장내 에 는 떠나가 ㄹ 듯 하 ㄴ 웃음 의 세계 가 아 지나 갔 다. [SEP]',\n",
       "  'score': 0.04863248020410538,\n",
       "  'token': 6581,\n",
       "  'token_str': '세계'},\n",
       " {'sequence': '[CLS] 장내 에 는 떠나가 ㄹ 듯 하 ㄴ 웃음 의 소리 가 아 지나 갔 다. [SEP]',\n",
       "  'score': 0.04228891059756279,\n",
       "  'token': 6609,\n",
       "  'token_str': '소리'},\n",
       " {'sequence': '[CLS] 장내 에 는 떠나가 ㄹ 듯 하 ㄴ 웃음 의 파도 가 아 지나 갔 다. [SEP]',\n",
       "  'score': 0.03808552771806717,\n",
       "  'token': 16893,\n",
       "  'token_str': '파도'},\n",
       " {'sequence': '[CLS] 장내 에 는 떠나가 ㄹ 듯 하 ㄴ 웃음 의 노래 가 아 지나 갔 다. [SEP]',\n",
       "  'score': 0.034016989171504974,\n",
       "  'token': 14829,\n",
       "  'token_str': '노래'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(preproc('장내에는 떠나갈듯한 웃음의 MASK 가 지나갔다.')) # 파도 (횃불을 찾아서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 빛 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.2698248326778412,\n",
       "  'token': 6458,\n",
       "  'token_str': '빛'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 눈물 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.08702857047319412,\n",
       "  'token': 15260,\n",
       "  'token_str': '눈물'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 꽃 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.07096227258443832,\n",
       "  'token': 5622,\n",
       "  'token_str': '꽃'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 웃음 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.04789747670292854,\n",
       "  'token': 7016,\n",
       "  'token_str': '웃음'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 눈 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.018613819032907486,\n",
       "  'token': 5745,\n",
       "  'token_str': '눈'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 불길 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.013383710756897926,\n",
       "  'token': 15242,\n",
       "  'token_str': '불길'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 얼굴 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.013291474431753159,\n",
       "  'token': 14959,\n",
       "  'token_str': '얼굴'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 불 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.012859541922807693,\n",
       "  'token': 6424,\n",
       "  'token_str': '불'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 눈길 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.012544226832687855,\n",
       "  'token': 15352,\n",
       "  'token_str': '눈길'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 자욱 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.011708377860486507,\n",
       "  'token': 15659,\n",
       "  'token_str': '자욱'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 모습 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.010853804647922516,\n",
       "  'token': 14894,\n",
       "  'token_str': '모습'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 하늘 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.010539374314248562,\n",
       "  'token': 7796,\n",
       "  'token_str': '하늘'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 바람 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.009536693803966045,\n",
       "  'token': 6275,\n",
       "  'token_str': '바람'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 불빛 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.008470797911286354,\n",
       "  'token': 17634,\n",
       "  'token_str': '불빛'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 별 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.0072400798089802265,\n",
       "  'token': 6356,\n",
       "  'token_str': '별'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 동음 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.006757629103958607,\n",
       "  'token': 17620,\n",
       "  'token_str': '동음'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 박동 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.005931236781179905,\n",
       "  'token': 20032,\n",
       "  'token_str': '박동'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 땀 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.005716376472264528,\n",
       "  'token': 5963,\n",
       "  'token_str': '땀'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 정 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.005527300760149956,\n",
       "  'token': 7227,\n",
       "  'token_str': '정'},\n",
       " {'sequence': '[CLS] 향 옥 의 눈 이 별 같이 빛나 며 웃음 의 불꽃 이 튕기 었 다. [SEP]',\n",
       "  'score': 0.005137578118592501,\n",
       "  'token': 18855,\n",
       "  'token_str': '불꽃'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask.topk = 20\n",
    "fill_mask(preproc('향옥의 눈이 별같이 빛나며 웃음의 MASK 이 튕겼다.')) # 불꽃  (평양의 봉화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] 웃음 의 빛 을 떨기 떨기 피우 ㄹ 때 [SEP]',\n",
       "  'score': 0.09966671466827393,\n",
       "  'token': 6458,\n",
       "  'token_str': '빛'},\n",
       " {'sequence': '[CLS] 웃음 의 꽃 을 떨기 떨기 피우 ㄹ 때 [SEP]',\n",
       "  'score': 0.06703218072652817,\n",
       "  'token': 5622,\n",
       "  'token_str': '꽃'},\n",
       " {'sequence': '[CLS] 웃음 의 눈물 을 떨기 떨기 피우 ㄹ 때 [SEP]',\n",
       "  'score': 0.06487768888473511,\n",
       "  'token': 15260,\n",
       "  'token_str': '눈물'},\n",
       " {'sequence': '[CLS] 웃음 의 웃음 을 떨기 떨기 피우 ㄹ 때 [SEP]',\n",
       "  'score': 0.05393465608358383,\n",
       "  'token': 7016,\n",
       "  'token_str': '웃음'},\n",
       " {'sequence': '[CLS] 웃음 의 눈 을 떨기 떨기 피우 ㄹ 때 [SEP]',\n",
       "  'score': 0.047109223902225494,\n",
       "  'token': 5745,\n",
       "  'token_str': '눈'},\n",
       " {'sequence': '[CLS] 웃음 의 땀 을 떨기 떨기 피우 ㄹ 때 [SEP]',\n",
       "  'score': 0.04176807776093483,\n",
       "  'token': 5963,\n",
       "  'token_str': '땀'},\n",
       " {'sequence': '[CLS] 웃음 의 얼굴 을 떨기 떨기 피우 ㄹ 때 [SEP]',\n",
       "  'score': 0.028608063235878944,\n",
       "  'token': 14959,\n",
       "  'token_str': '얼굴'},\n",
       " {'sequence': '[CLS] 웃음 의 눈길 을 떨기 떨기 피우 ㄹ 때 [SEP]',\n",
       "  'score': 0.024200644344091415,\n",
       "  'token': 15352,\n",
       "  'token_str': '눈길'},\n",
       " {'sequence': '[CLS] 웃음 의 불 을 떨기 떨기 피우 ㄹ 때 [SEP]',\n",
       "  'score': 0.01604815013706684,\n",
       "  'token': 6424,\n",
       "  'token_str': '불'},\n",
       " {'sequence': '[CLS] 웃음 의 가슴 을 떨기 떨기 피우 ㄹ 때 [SEP]',\n",
       "  'score': 0.015496322885155678,\n",
       "  'token': 14807,\n",
       "  'token_str': '가슴'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask.topk = 10\n",
    "fill_mask(preproc('웃음의 MASK 을 떨기떨기 피울 때')) # 꽃 (리영복 - 탐사대원의 노래)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 빛 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.24179239571094513,\n",
       "  'token': 6458,\n",
       "  'token_str': '빛'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 꽃 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.0679134726524353,\n",
       "  'token': 5622,\n",
       "  'token_str': '꽃'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 웃음 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.05536432936787605,\n",
       "  'token': 7016,\n",
       "  'token_str': '웃음'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 모습 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.029855893924832344,\n",
       "  'token': 14894,\n",
       "  'token_str': '모습'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 자욱 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.025560587644577026,\n",
       "  'token': 15659,\n",
       "  'token_str': '자욱'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 눈 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.02507529966533184,\n",
       "  'token': 5745,\n",
       "  'token_str': '눈'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 눈물 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.021939940750598907,\n",
       "  'token': 15260,\n",
       "  'token_str': '눈물'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 하늘 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.020553400740027428,\n",
       "  'token': 7796,\n",
       "  'token_str': '하늘'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 얼굴 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.020347343757748604,\n",
       "  'token': 14959,\n",
       "  'token_str': '얼굴'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 눈길 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.009106932207942009,\n",
       "  'token': 15352,\n",
       "  'token_str': '눈길'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 감정 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.008671581745147705,\n",
       "  'token': 15190,\n",
       "  'token_str': '감정'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 불빛 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.0075676157139241695,\n",
       "  'token': 17634,\n",
       "  'token_str': '불빛'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 색 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.007212256081402302,\n",
       "  'token': 6538,\n",
       "  'token_str': '색'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 흐름 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.006060019135475159,\n",
       "  'token': 15689,\n",
       "  'token_str': '흐름'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 박동 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.005728991702198982,\n",
       "  'token': 20032,\n",
       "  'token_str': '박동'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 싹 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.005690494552254677,\n",
       "  'token': 6754,\n",
       "  'token_str': '싹'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 뜻 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.0052561406046152115,\n",
       "  'token': 5997,\n",
       "  'token_str': '뜻'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 가슴 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.004956057760864496,\n",
       "  'token': 14807,\n",
       "  'token_str': '가슴'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 불길 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.0049355970695614815,\n",
       "  'token': 15242,\n",
       "  'token_str': '불길'},\n",
       " {'sequence': '[CLS] 그날 의 회포 에 잠기 어서 이 ㄴ지 주름살 마다 웃음 의 맛 을 그리 어 가 았 다. [SEP]',\n",
       "  'score': 0.004527512937784195,\n",
       "  'token': 6164,\n",
       "  'token_str': '맛'}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask.topk = 20\n",
    "fill_mask(preproc('그날의  회포에  잠겨서인지  주름살마다  웃음의  MASK 을  그려갔다.')) # 꽃 (리영복 - 탐사대원의 노래)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_args_parser',\n",
       " '_forward',\n",
       " '_parse_and_tokenize',\n",
       " 'binary_output',\n",
       " 'default_input_names',\n",
       " 'device',\n",
       " 'device_placement',\n",
       " 'ensure_exactly_one_mask_token',\n",
       " 'ensure_tensor_on_device',\n",
       " 'framework',\n",
       " 'model',\n",
       " 'modelcard',\n",
       " 'predict',\n",
       " 'save_pretrained',\n",
       " 'tokenizer',\n",
       " 'topk',\n",
       " 'transform']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(fill_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3e8cd5fdef80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfill_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "fill_mask.topk(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute word probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0930 10:18:12.214763 24936 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\\Users\\bber2012/.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0930 10:18:12.215761 24936 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0930 10:18:13.097404 24936 tokenization_utils_base.py:1254] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\bber2012/.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0930 10:18:14.036892 24936 configuration_utils.py:264] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\\Users\\bber2012/.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0930 10:18:14.037890 24936 configuration_utils.py:300] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0930 10:18:14.351052 24936 modeling_utils.py:667] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at C:\\Users\\bber2012/.cache\\torch\\transformers\\f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "W0930 10:18:17.695113 24936 modeling_utils.py:757] Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "W0930 10:18:17.696110 24936 modeling_utils.py:768] Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"This is a [MASK] to test if BERT is any good as a model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -6.7330,  -6.6794,  -6.6839,  ...,  -6.1019,  -5.8115,  -4.0742],\n",
      "         [-14.7104, -14.8737, -14.7737,  ..., -14.2241, -12.3348, -10.9089],\n",
      "         [-10.0567, -10.0685, -10.1663,  ...,  -9.8117,  -6.8159,  -7.0186],\n",
      "         ...,\n",
      "         [ -6.5769,  -6.4774,  -6.5857,  ...,  -6.6674,  -6.2985,  -5.4500],\n",
      "         [ -5.4542,  -5.3873,  -5.5095,  ...,  -5.9345,  -4.9863,  -2.1786],\n",
      "         [-10.8866, -10.7399, -10.7980,  ...,  -8.9335,  -8.9119,  -6.6197]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['way']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "token_logits = model(input_ids)[0]\n",
    "print(token_logits)\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "predicted_index = torch.argmax(token_logits[0, mask_token_index]).item()\n",
    "tokenizer.convert_ids_to_tokens([predicted_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -6.6894,  -6.6324,  -6.6375,  ...,  -5.9988,  -5.7349,  -4.0297],\n",
      "         [-13.3815, -13.5201, -13.3391,  ..., -13.3838, -10.7279,  -8.4036],\n",
      "         [ -7.8987,  -7.8683,  -7.9880,  ...,  -8.0206,  -5.1704,  -5.3737],\n",
      "         ...,\n",
      "         [-14.1365, -14.2509, -13.9203,  ..., -13.8206, -10.6771,  -9.3580],\n",
      "         [ -7.9741,  -7.9602,  -8.1151,  ...,  -7.5900,  -6.9989,  -5.7743],\n",
      "         [-10.6436, -10.4992, -10.4450,  ...,  -8.2639,  -8.4850,  -6.8674]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['way']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "token_logits = model(input_ids)[0]\n",
    "print(token_logits)\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "predicted_index = torch.argmax(token_logits[0, mask_token_index]).item()\n",
    "tokenizer.convert_ids_to_tokens([predicted_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\WPy64-3740\\python-3.7.4.amd64\\lib\\site-packages\\transformers\\pipelines.py:882: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n",
      "  masked_index = (input_ids == self.tokenizer.mask_token_id).nonzero()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': '[CLS] this is a way to test bert [SEP]',\n",
       "  'score': 0.40268340706825256,\n",
       "  'token': 2126,\n",
       "  'token_str': 'way'},\n",
       " {'sequence': '[CLS] this is a time to test bert [SEP]',\n",
       "  'score': 0.04881566762924194,\n",
       "  'token': 2051,\n",
       "  'token_str': 'time'},\n",
       " {'sequence': '[CLS] this is a test to test bert [SEP]',\n",
       "  'score': 0.025697337463498116,\n",
       "  'token': 3231,\n",
       "  'token_str': 'test'},\n",
       " {'sequence': '[CLS] this is a lot to test bert [SEP]',\n",
       "  'score': 0.019878488034009933,\n",
       "  'token': 2843,\n",
       "  'token_str': 'lot'},\n",
       " {'sequence': '[CLS] this is a trick to test bert [SEP]',\n",
       "  'score': 0.017333118245005608,\n",
       "  'token': 7577,\n",
       "  'token_str': 'trick'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
